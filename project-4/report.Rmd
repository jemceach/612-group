---
title: "Accuracy and Beyond in Restaurant Recommender Systems"
author: "Christina Valore, Juliann McEachern, & Rajwant Mishra"
date: "July 2, 2019"
output:
  prettydoc::html_pretty:
    theme: leonids 
    highlight: github
---

```{r dependencies, echo=F,comment=F,message=F,warning=F,prompt=F}
# data processing
library(tidyr); library(dplyr); library(RCurl); library(jsonlite); library(plyr)

#formatting
library(knitr); library(kableExtra); library(prettydoc); library(default)

#visualization
library(ggplot2)

#collaborative recommender 
library(recommenderlab); 

#hybrid recommender
library(gbm);library(rsample);library(Metrics);library(caret);library(textmineR)

# global options
## knit sizing
options(max.print="100"); opts_knit$set(width=75) 

## augment chunk output
opts_chunk$set(echo=F,cache=F, tidy=T,comment=F,message=T,warning=T) #change message/warning to F upon completion

## set table style for consistency
default(kable) <- list(format="html")
default(kable_styling)  <- list(bootstrap_options = "hover",full_width=T, font_size=10)
default(scroll_box) <- list(width = "100%")

## working directory
##try(setwd("~/Github/612-group/project-4"))
```


# Overview

The goal of this assignment is give you practice working with accuracy and other recommender system metrics. 

**Assignment Tasks** (SUMMARIZE OUR PROCESS LATER):

1. Compare the accuracy of at least two recommender system algorithms against your offline data. 
2. Compare and report on any change in accuracy before and after you've made the change in the algorithms. 
3. Include at least one business or user experience goal such as increased serendipity, novelty, or diversity. 
4. Textual conclusion.

**GROUP NOTES** (DELETE LATER):

*  3 Matrices are defined. 1 has null values, 1 is imputed with the mean, 1 is imputed with 0. We do not have to use all of these. Just a starting place. 
*  Dataset includes many interesting categorical features. We could try 2 user-item algorthims and a hybrid approach.
*  Lets try to do our best to minimize code chunks, add explanitory text for readers and within code chunks for us to follow.
*  Add/reference sources at the end! 
*  Set important code chunks to `echo=T` if you want the reader to see the process.

## Data Selection

Our data was sourced from Kaggle's [Restaurant Data with Consumer Ratings](https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings) collection, which contained several datasets pertaining to restaurants and their patrons. The csv files are stored within our repository in the data folder. 

```{r}
data1<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/chefmozcuisine.csv") # cusine tags
data2<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/geoplaces2.csv") # restaurant name
data3<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/rating_final.csv") # user ratings
data4<-read.csv("https://raw.githubusercontent.com/jemceach/612-group/master/project-4/data/userprofile.csv") # user profile
```

## Data Tranformations

We cleaned our data using transformations and regular expression unite our user and restaurant data. The output of which can be previewed below: 

```{r}
# restaurant dataframe

cuisine <- aggregate(Rcuisine ~., data1, toString) # concat cuisine types into tags



## merge & transform

restaurant <- data2 %>% select(placeID, name, city,price, alcohol,smoking_area) %>% mutate(name=tolower(gsub("[\u00ef\u00bf\u00bd\'_']", " ", name))) %>% mutate(city=tolower(city)) 

restaurant$price <- factor(restaurant$price, levels = c("low", "medium", "high")) # set factor

restaurant$city <- revalue(restaurant$city, c("cd victoria"="ciudad victoria", "cd. victoria"="ciudad victoria","victoria "="ciudad victoria","victoria"="ciudad victoria","san luis potosi "="san luis potosi","san luis potos"="san luis potosi","s.l.p"="san luis potosi","slp"="san luis potosi","s.l.p."="san luis potosi")) # rename cities for analysis



## change for TFIDF analysis

restaurant$smoking_area = revalue(restaurant$smoking_area, c("none"="nonsmoking", "not permitted"="nonsmoking", "section"="smoking", "permitted"="smoking", "only at bar"="smoking")) 

restaurant$price = revalue(restaurant$price, c("low"="lowprice", "medium"="mediumprice", "high" ="highprice"))

restaurant$alcohol = revalue(restaurant$alcohol, c("No_Alcohol_Served"="nonalcoholic", "Full_Bar"="fullbar", "Wine-Beer" ="winebeer"))

restaurant$city<-factor(restaurant$city, exclude = NULL)

restaurant <- inner_join(restaurant, cuisine, by="placeID")



# user dataframe: 

##select attributes of interest from profile

user_profile <- data4 %>% select(userID, budget,activity, drink_level, smoker)

user <- inner_join(data3,user_profile,by='userID') 

## set factors

user$budget <- revalue(user$budget, c("low"="lowbudget", "medium"="mediumbudget","high"="highbudget"))

user$budget <- factor(user$budget, levels = c("lowbudget", "mediumbudget", "highbudget"))

user$smoker <- revalue(user$smoker, c("false"="nonsmoker", "true"="smoker")) 

user$smoker <- factor(user$smoker, levels = c("nonsmoker", "smoker"))





## change ratings from 0-2 scale to 1-3

user$rating[user$rating==2]<-3;user$rating[user$rating==1]<-2;user$rating[user$rating==0]<-1

user$service_rating[user$service_rating==2]<-3;user$service_rating[user$service_rating==1]<-2;user$service_rating[user$service_rating==0]<-1

user$food_rating[user$food_rating==2]<-3;user$food_rating[user$food_rating==1]<-2;user$food_rating[user$food_rating==0]<-1;



# combine user / restaurant data &  subset 

data <- inner_join(user, restaurant, by="placeID") 



data <- data %>% filter(city == "san luis potosi", activity=="student") %>% select(-city, -activity)



data %>% head() %>% kable(caption="User-Item Dataframe") %>% kable_styling() 
```


## Data Exploration

We found that 80% of our raters were students students and 76% of our restaurants were located within the Mexican city of San Luis Potosi.  As a result, we subsetted our restaurant/patron data to limit the scope of our system to this specific population. After subsetting our raw data, we identified 78 unique users and 56 restaurants to build our recommender systems from. 

The following plots help visualize the distribution of our overall ratings given by users based on their budget and the restaurant's categorized pricing. We also viewed the rating counts each restaurant received. On average, each venue received 13 user ratings. 

```{r, fig.height=2, out.width = '100%'}
data %>% filter(!is.na(budget)) %>% group_by(rating) %>% ggplot(aes(x=rating)) + geom_histogram(bins=3, color='#000000', fill='#e4d1d1') +labs(title="Raw Ratings Distribution by Patron Budget") + facet_wrap(~budget, nrow=1)

ggplot(data, aes(x=rating)) + geom_histogram(bins=3, color='#000000', fill='#b9b0b0') +labs(title="Raw Ratings Distribution by Restaurant Pricing") + facet_wrap(data$price, nrow=1)
```

```{r,fig.height=3, out.width = '100%'}
data %>% mutate(placeID = as.factor(placeID), rating=as.factor(rating)) %>% group_by(placeID) %>% add_tally() %>% ungroup() %>% ggplot(aes(x=reorder(placeID, -n),fill=rating))+ geom_bar(stat="count", color="#686256") + theme(axis.title.x=element_blank(),axis.text.x=element_blank(),axis.ticks.x=element_blank())+labs(title="Restaurant Rating Counts") + scale_fill_manual(values=c("#f0efef", "#e4d1d1", "#b9b0b0"))
```

```{r, fig.height=4.5, out.width = '100%'}
# Cuisine bar plot. May or may not include. 
data %>% select(Rcuisine, rating) %>% mutate(rating=as.factor(rating)) %>% group_by(Rcuisine) %>% add_tally() %>% ungroup %>% ggplot(aes(x=reorder(Rcuisine, -n), fill=rating)) + geom_bar(stat="count",color="#686256")+labs(title="Restaurant Cuisine Tag Counts with Ratings") + theme(axis.title.x=element_blank(), axis.text.x = element_text(angle = 45, hjust = 1))+ scale_fill_manual(values=c("#f0efef", "#e4d1d1", "#b9b0b0"))
```

# Collaborative Filtering 

**ADD TEXT EXPLINATION OF BUSINESS/USER GOALS: Serendipity? Goal to be recommending relevant items to targeted users that are different then items the user has already rated?**

## Matrix Building 

We further transformed our data into a user-item matrix to build our recommender systems.

```{r}
# create user item matrix
ui_matrix <- data %>% select(userID, placeID, rating) %>% spread(placeID, rating)
rownames(ui_matrix)<-ui_matrix$userID # set row names to userid
ui_matrix<-ui_matrix %>% select(-userID) %>% as.matrix()# remove userid from columns
umat <- as(ui_matrix,"realRatingMatrix") # save real ratings for algo 



##### built two other frames for comparison - mu = mean ratings imputation; na = imputed with 0 
ui_mtx_mu <- ui_matrix; ui_mtx_na <- ui_matrix 

for(i in c(1:56)){
  ui_mtx_mu[is.na(ui_mtx_mu[,i]), i] <- round(mean(ui_mtx_mu[,i], na.rm = TRUE))
}

ui_mtx_na[is.na(ui_mtx_na)] <- 0

## preview matrices (DELETE UNUSED LATER)
as.data.frame.array(ui_matrix) %>% head() %>% kable(caption="User Matrix Preview (Ratings Preserved as NA)") %>% kable_styling()%>% scroll_box()
as.data.frame.array(ui_matrix) %>% head() %>% kable(caption="User Matrix Preview (NA=Mean Rating)") %>% kable_styling()%>% scroll_box()
as.data.frame.array(ui_matrix) %>% head() %>% kable(caption="User Matrix Preview (NA=0)") %>% kable_styling()%>% scroll_box()
```

## Training and Test Subsets

Our data was split into training and tests sets for model evaluation of both two recommender algorithms. We split our data using the `recommenderlab` package. 90% of data was retained for training and 10% for testing purposes.

```{r}
# evaluation method using training set of 
evalu <- evaluationScheme(umat, method="split", train=0.9, given=3, goodRating=0)
```


## Algorithm 1: User-Based (Christina)

ADD TEXT ETC. 

### Process 
Text
```{r, echo=T}
#non-normalized
ub_n <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = NULL, method="Cosine"))

#centered
ub_c <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = "center",method="Cosine"))

#Z-score normalization
ub_z <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = "Z-score",method="Cosine"))
```

### Predictions 
Text
```{r}
#predicted ratings
p1 <- predict(ub_n, getData(evalu,"known"),type="ratings")
p2 <- predict(ub_c, getData(evalu,"known"),type="ratings")
p3 <- predict(ub_z, getData(evalu,"known"),type="ratings")

#ceiling and floor values
p1@data@x[p1@data@x[] < 1] <- 1
p1@data@x[p1@data@x[] > 3] <- 3

p2@data@x[p2@data@x[] < 1] <- 1
p2@data@x[p2@data@x[] > 3] <- 3

p3@data@x[p3@data@x[] < 1] <- 1
p3@data@x[p3@data@x[] > 3] <- 3

#compare the predictions using the different normalize methods
error_UCOS <- rbind(
  ub_n = calcPredictionAccuracy(p1, getData(evalu, "unknown")),
  ub_c = calcPredictionAccuracy(p2, getData(evalu, "unknown")),
  ub_z = calcPredictionAccuracy(p3, getData(evalu, "unknown"))
)
error_UCOS %>% kable(caption="Prediction Comparisons") %>% kable_styling()
```

## Algorithm 2 (Raj)


## Compare 
Compare algorithms.

# Hybrid Recommender System (Juliann)

Our data source was rich with categorical features containing user data such as height, weight, religion, personality-types and more.  We wanted to also try to incorporate a hybrid recommender to look at user and restaurant attributers to build our prediction model using a different machine learning techinique -- Gradient Boosting. 

Reference later for [hybrid model algorithms](https://medium.com/@alfonsollanes/how-do-you-measure-and-evaluate-the-quality-of-recommendation-engines-2e91db5952af). 


## Gradient Boosted Model (GBM)

The gradient boosted technique evaluates our data from decision trees. Using an iterative process, the algoritm improves the fit of our inputted variables when creating our predictive model.  We used the `gbm`, `resample`, `Metrics` & `caret` packages to build and evaluate the model.

### Build GBM Model 

Our GBM model measures the relative influence of our select restaurant and  user characteristics to make user-rating predictions. We found that type of cuisine from our tags had significant importance on our model.

```{r}
# Build Model

## subset data for model
PatronDataModel <- data %>% select(userID, placeID,rating, price,alcohol,smoking_area, Rcuisine)

## factor data
PatronDataModel$Rcuisine<-as.factor(PatronDataModel$Rcuisine)

## train test spit
PatronSplit <- initial_split(PatronDataModel, prop = .9)
PatronTrain <- training(PatronSplit)
PatronTest  <- testing(PatronSplit)

## fit model
patron_fit_1 <- gbm::gbm(rating ~., 
             data = PatronDataModel, 
             distribution = "gaussian", #gaussian algo (squared error)
             verbose = F, #change to true to view output
             shrinkage = 0.01, # step size reduction
             interaction.depth = 3, # tree depth for two way interactions
             n.minobsinnode = 5, # min observations
             n.trees = 500, #number of trees to fit/iterate
             cv.folds = 10 #cv folds
             )

show.gbm(patron_fit_1)

## summarize model
summary.gbm(patron_fit_1, plotit = F) %>% kable() %>%kable_styling()

# Predictions

## optimal number of boosting iterations
perf_gbm1 = gbm.perf(patron_fit_1, method = "cv", plot.it = F)

## predict
patron_prediction_1 <- stats::predict(
                          object = patron_fit_1, # fit prediction
                          newdata = PatronTest, # test data 
                          n.trees = perf_gbm1) # optimal n of iterations
```


### Evaluation

We evaluated GBM model using standard RMSE, MSE, MEA, and RMSLE calculations. We also used a confusion matrix to calculate the accuracy of our predictions. 

```{r}
# Metrics
fit_rmse <- Metrics::rmse(actual = PatronTest$rating, predicted =patron_prediction_1)
fit_mse <- Metrics::mse(actual = PatronTest$rating, predicted =patron_prediction_1)
fit_mae <- Metrics::mae(actual = PatronTest$rating, predicted =patron_prediction_1)
fit_rmsle <- Metrics::rmsle(actual = PatronTest$rating, predicted =patron_prediction_1)

#set factors
PatronTest$rating<-as.factor(PatronTest$rating)
fit_predict <- as.factor(round(patron_prediction_1))

#confusion matrix
eval_cm <- confusionMatrix(fit_predict, PatronTest$rating)
fit_accuracy<-round(eval_cm$overall[1],4)

# view metrics
results <- cbind(fit_rmse, fit_mse,fit_mae,fit_rmsle,fit_accuracy)
results %>% kable(caption="Model Evaluation Metrics") %>% kable_styling()

### Note: Output shows NA (not meaningful for factors) if chunk is rerun without clearing global environment
```

The confusion matrix from the `caret` package also allowed us to look at the accuracy and percision by each of our predicted rating as a unique class.  

```{r}
eval_cm$byClass %>% round(4) %>% kable(caption = "Confusion Matrix Evaluation by Rating") %>% kable_styling()
```

### Analysis 

Pros:

*  Novelty: Relevant recommendations not based on popularity.

Cons:

*  Over-specialisation
*  May fail diversity test 

Add more later :)



# Conclusion

**As part of your textual conclusion, discuss one or more additional experiments that could be performed and/or metrics that could be evaluated only if online evaluation was possible. Also, briefly propose how you would design a reasonable online evaluation environment.**


Also: Compare content algorith outcomes to hybrid approach. 

# References:

[**Building Recommenders**:](https://buildingrecommenders.wordpress.com/2015/11/19/overview-of-recommender-algorithms-part-3/) Overview of Recommender Algorithms

[**recommenderlab**:](https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf) A Framework for Developing and Testing Recommendation Algorithms 

[**UC Business Analytics R Programming Guide**:](http://uc-r.github.io/gbm_regression) Gradient Boosting Machines
