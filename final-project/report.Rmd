---
title: "Final Project - Yelp Recommender System"
author: "Juliann McEachern, Rajwant Mishra,Christina Valore"
date:  " July 16, 2019"
output: 
  html_document:
    theme: paper
    highlight: pygments
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged
    code_folding: hide
---
# Overview 

Our project used data from Kaggle's 2013 Yelp Challenge. This challenge included a subset of Yelp data from the metropolitan area of Phoenix, Arizona. Our data takes into account user reviews, ratings, and check-in data for a wide-range of businesses. 

```{r dependencies,echo=F,comment=F,message=F,warning=F,prompt=F, cache=T}
## data processing packages
library(dplyr);library(tibble);library(forcats)

##formatting packages
library(knitr); library(kableExtra); library(default)

##visualization packages
library(ggplot2)

##recommender packages
library(recommenderlab); library(Metrics); library(lsa); library(diveRsity);library(purrr)

## Sparklyr 
#install.packages("devtools")
#devtools::install_github("rstudio/sparklyr")
#spark_install(version = "2.4.3")
library(sparklyr)

##preprocessing data
#suppressWarnings(source("preprocessing.R"))

# global options

## knit sizing
options(max.print="100"); opts_knit$set(width=75) 

## augment chunk output
opts_chunk$set(echo=T,cache=F, tidy=T,comment=F,message=T,warning=T) #change message/warning to F upon completion

## set table style for consistency
default(kable) <- list(format="html")
default(kable_styling)  <- list(bootstrap_options = "hover",full_width=T, font_size=10)
default(scroll_box) <- list(width = "100%")
```


## Data Aquisition & Transformations {.tabset .tabset-fade .tabset-pills}

Data was acquired and transformed in the `preprocessing.R` file located within our repositories final-project folder. Our data source was provided as multiarray Json files, meaning each file is a collection of json data. We used  `stream_in` function, which parses json data line-by-line from the data folder of our repository. The collections included three, large data for Yelp businesses, users, and reviews. 

Once obtained, we prepared our data for our recommender system using the following transformations:

### Business

We choose to limit the scope to our recommender system to only businesses with tags related to food and beverages. There were originally 508 unique category tags listed within our business data. We manually filtered 112 targeted categories to subset our data. 

We applied additional transformation to remove unnessacary data. There were 1,224 business in our data that were permanently closed. These companies accounted for 9.8% of all businesses, which were subsequently removed from our data. There were also 3 businesses in our dataset from outside of AZ that we also removed. 

As a result of our transformations, our recommender data was shortened 4,828 unique businesses. This was further limited to 4,332 after randomly sampling our user-data. The output of which can be previewed below: 

```{r business, echo=F}
#preview business
business %>% head() %>% kable(caption="Preview Business Data") %>% kable_styling()
```

### Review

We subset our review data from the subset of food and beverage businesses. This dropped our review data from 229,907 to 165,823 reviews. We later applied another filter to the data to only use reviews from 10,000 randomly sampled users. This further decreases reviews to 44,494 observations. Our review data can be previewed in two parts below:

```{r review, echo=F}
#preview reviews
review %>% select() %>% head(1) %>% kable(caption="Preview Review Data (without Review Text)") %>% kable_styling()
```

### User 

Next, we applied a similar filter to users to subset our data based on only our selected businesses. This decreased our user data from 43,873 to 35,268 distinct user_id observations. Do to processing constraints in R, we choose to randomly sample 10,000 users from these unique profiles. 

The dataframe preview below shows aggregate user data for all reviews an individual user provided for yelp within our data selection. 

```{r user, echo=F}
#preview users
user %>% head() %>% kable(caption="Preview User Data") %>% kable_styling()
```

### Merged Dataframe 

Last, we created our main dataframe by merging business and reviews on `Business_ID`. This dataframe will serve as the source of data for our recommender algorithms. The user and business unique keys were simplified from characters to numeric user/item identifiers. 

This dataframe will be referenced later on when building our recommender matrices and algorithms. 

```{r df, echo=F}
df %>% select(-text,-business_id, -user_id,-review_id) %>% head() %>% kable(caption="Preview main dataframe") %>% kable_styling()
```

# Recommender Algorithm

We tested recommender algorithms using `recommenderlab` and `sparklyr` to see which performed the best on our recommender system data. To test the algorithsm, we first had to create a user-item matrix and then split our data into training and test sets. 

**Matrix Building**

We converted our raw ratings data into a user-item matrix to test and train our subsequent recommender system algorithms. The matrix was saved as a realRatingMatrix for processing purposes later on using the `recommenderlab` package. 

The matrix data can be viewed below. 

```{r matrix-building, cache=T}
# spread data from long to wide format 
matrix_data <- df %>% select(userID, itemID, stars) %>% spread(itemID, stars)
# set row names to userid
rownames(matrix_data)<-matrix_data$userID 
# remove userid from columns 
matrix_data <-matrix_data %>% select(-userID) 
# convert to matrix
ui_mat <- matrix_data %>% as.matrix()
# store matrix as realRatingMatrix
ui_mat <- as(ui_mat,"realRatingMatrix")
```

```{r view-matrix, echo=F}
# view matrix data 
matrix_data %>% head() 
```


**Train and Test Splits**

Our data was split into training and tests sets for model evaluation of both two recommender algorithms. We split our data with 10 k-folds using the `recommenderlab` package. 80% of data was retained for training and 20% for testing purposes.

```{r train-test}
# evaluation method with 80% of data for train and 20% for test
set.seed(0)

evalu <- evaluationScheme(ui_mat, method="split", train=0.8, given=1, goodRating=1, k=10)

# prep data
train <- getData(evalu, 'train')# Training Dataset 
dev_test <- getData(evalu, 'known') # Test data from evaluationScheme of type KNOWN
test <- getData(evalu, 'unknown') # Unknow datset used for RMSE / model evaluation
```


## RecommenderLab {.tabset .tabset-fade .tabset-pills}

### User-based CF

In our first example, user-based CF is used to create recommendations with the recommeder lab package in R. We start by training our recommender with the train set, with our data being normalized with the Z-score and using cosine similarity for comparisons. 

We then create our predictions using the dev-test set with ratings as our prediction output. It is imperative to set a floor and ceiling as sometimes predictions will fall outside of our ratings scale of 1-5.

Finally we calculated the prediction accuracy against the test data
```{r ub, cache=T}
# using recommender lab, create UBCF recommender with z-score normalized data using cosine similarity
UB <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = "Z-score",method="Cosine"))

# create rating predictions and store
p <- predict(UB, getData(evalu, "known"), type="ratings")

# set floor and ceiling for ratings that fall outside scale
p@data@x[p@data@x[] < 1] <- 1
p@data@x[p@data@x[] > 5] <- 5

# calculate the prediction accurary based on our test data
UB_acc <- calcPredictionAccuracy(p, getData(evalu, "unknown"))
UB_acc
```

### Item-based CF

This is the same process as above except this time we are using an item-based CF to create our recommendations.

```{r ib, cache=T}
IB <- Recommender(getData(evalu, "train"), "IBCF", 
      param=list(normalize = "Z-score",method="Cosine"))

p1 <- predict(IB, getData(evalu, "known"), type="ratings")

p1@data@x[p1@data@x[] < 1] <- 1
p1@data@x[p1@data@x[] > 5] <- 5

IB_acc <- calcPredictionAccuracy(p1, getData(evalu, "unknown"))
IB_acc
```

### Results
```{r}
# print out errors in table
error <- rbind(
UB_acc,
IB_acc
)

kable(error)
```


## Sparklyr {.tabset .tabset-fade .tabset-pills}

Due to the size of our data, we choose to use Spark in R to avoid input/output (I/O) bottleneck issues and maximize the performance speed of our recommender algorithm calculations.

### Connecting

We initiated a local connection with Spark (V2.4.3). Our yelp data was inputted into a spark table and split for training and testing purposes. We uploaded our training and test splits to minimize the variance in our comparisons.

```{r initiate-spark}
# configure spark connection
config <- spark_config()
config$spark.executor.memory <- "8G"
config$spark.executor.cores <- 2
config$spark.executor.instances <- 3
config$spark.dynamicAllocation.enabled <- "false"

# initiate connection
sc <- spark_connect(master = "local", config=config, version = "2.4.3")

# unhash to verify version: 
# spark_version(sc)

# select data for spark and create spark table 
spark_train <- as(train,"data.frame")
spark_test<- as(test,"data.frame") 

spark_train <- sdf_copy_to(sc, spark_train, "spark_train", overwrite = TRUE) 
spark_test <- sdf_copy_to(sc, spark_test, "spark_test", overwrite = TRUE) 

# Transform features
spark_train <- spark_train %>%
  ft_string_indexer(input_col = "user", output_col = "user_index") %>%
  ft_string_indexer(input_col = "item", output_col = "item_index") %>%
  sdf_register("spark_train")
  
spark_test <- spark_test %>%
  ft_string_indexer(input_col = "user", output_col = "user_index") %>%
  ft_string_indexer(input_col = "item", output_col = "item_index") %>%
  sdf_register("spark_test")
```

### ALS 

Once connected, we applied the alternating least squares (ALS) for our recommender predictions.

```{r spark-als}
# build model using user/business/ratings
als_fit <- ml_als(spark_train, max_iter = 5, nonnegative = TRUE, 
                   rating_col = "rating", 
                   user_col = "user_index", 
                   item_col = "item_index")

# predict from the model for the training data
als_predict_train <- ml_predict(als_fit, spark_train) %>% collect()
als_predict_test <- ml_predict(als_fit, spark_test) %>% collect()

# Remove NaN (result of test/train splits - not data)
als_predict_train <- als_predict_train[!is.na(als_predict_train$prediction), ] 
als_predict_test <- als_predict_test[!is.na(als_predict_test$prediction), ] 

# View results
als_predict_test %>% head() %>% kable() %>% kable_styling()
```

### Performance 

Our ALS calculations for RMSE, MSE, and MAE can be viewed below: 

```{r als-performance}
# Calculate RMSE/MSE/MAE 
als_mse_train <- mean((als_predict_train$rating - als_predict_train$prediction)^2)
als_rmse_train <- sqrt(als_mse_train)
als_mae_train <- mean(abs(als_predict_train$rating - als_predict_train$prediction))

als_mse_test <- mean((als_predict_test$rating - als_predict_test$prediction)^2)
als_rmse_test <- sqrt(als_mse_test)
als_mae_test <- mean(abs(als_predict_test$rating - als_predict_test$prediction))
```

```{r view-als, echo=F}
# View metrics 
als_train_metrics<- cbind(als_rmse_train,als_mse_train, als_mae_train) 
als_test_metrics<-cbind(als_rmse_test, als_mse_test, als_mae_test) 
als_metrics <- as.data.frame(rbind(als_train_metrics,als_test_metrics), row.names = c("ALS_train, ALS_test"))
als_metrics %>% rename(rmse = als_rmse_train, mse = als_mse_train, mae=als_mae_train)
```

### Recommend

The `ml_recommend` function allows us to see the top *n* user/item recommendations for each user/item. Below, we use this funcion and filtered our recommendations to show the top 10 restaurant recommendations for a selected user. 

```{r als-recommend}
als_user_recommend<-ml_recommend(als_fit, type="users", n=10)
```

```{r, echo=F}
als_user_recommend %>% head(10) %>% collect() %>% select(-recommendations)
```


```{r disconnect, echo=F}
# disconnect
spark_disconnect(sc)
```

## Algorithm with Binary Rating  {.tabset .tabset-fade .tabset-pills}

+ In the approach we will work with Bianry dataset and see how Business can be Recommended for the Given Business.


### Binary Rating Matrix

 + I first convert the ratings into a binary format to keep things simple. ratings of 4 and 5 are 
 + mapped to 1, representing likes, and ratings of 3 and below are mapped to 0, representing dislikes.
 
```{r}
# Columns are Busienss ID and Row is User_id
dat_binaryRatingMatrix <- df[,c(11,1,13)] %>%  mutate(Type = case_when(stars == 1 ~ 0,
                                       stars == 2 ~ 0,
                                       stars == 3 ~ 0,
                                       stars == 4 ~ 1,
                                       stars == 5 ~ 1,
                                       TRUE ~ 0)) %>% .[c(1,2,4)] %>% as("binaryRatingMatrix")

# Creation of Sample dataset. for Evalution Scheme
evalu_a2 <- evaluationScheme(dat_binaryRatingMatrix, method="split", train=0.9, given=1)

# Prep data
ratings_train <- getData(evalu_a2, "train")  # Training Dataset 
ratings_test_known <- getData(evalu_a2, "known")  # Test data from evaluationScheme of type KNOWN
ratings_test_unknown <- getData(evalu_a2, "unknown")  # Unknow datset used for RMSE / model

head(as(dat_binaryRatingMatrix[1:5,1:3],"matrix"))

```

### Evaluating the Model

+ In this section we are trying to see how "POPULAR" method and User Based Collaborative Filtering UBCF can recommend the top n items.
+ Its also important to note that Binary ratings are not well evaluated with the help of Precision and Recall .


```{r}
# creation of recommender model based on ubcf
Rec.ubcf <- Recommender(ratings_train, "UBCF")
#Creating POPULAR recommender model for comparison
Rec.pop <- Recommender(ratings_train, "POPULAR")
# making predictions on the test data set witj Top 2, 3, 5, and 10
pred_ubcf2 <- predict(Rec.ubcf, ratings_test_known, type="topNList",n=2)
pred_ubcf3 <- predict(Rec.ubcf, ratings_test_known, type="topNList",n=3)
pred_ubcf5 <- predict(Rec.ubcf, ratings_test_known, type="topNList",n=5)
pred_ubcf10 <- predict(Rec.ubcf, ratings_test_known, type="topNList",n=10)
# SAVE IT FOR LATTER USE
saveRDS(pred_ubcf2,"pred_ubcf2.rds")
saveRDS(pred_ubcf3,"pred_ubcf3.rds")
saveRDS(pred_ubcf5,"pred_ubcf5.rds")
saveRDS(pred_ubcf10,"pred_ubcf10.rds")

# making predictions on the test data set witj Top 2, 3, 5, and 10
pred_pop2 <- predict(Rec.pop, ratings_test_known, type="topNList",n=2)
pred_pop3 <- predict(Rec.pop, ratings_test_known, type="topNList",n=3)
pred_pop5 <- predict(Rec.pop, ratings_test_known, type="topNList",n=5)
pred_pop10 <- predict(Rec.pop, ratings_test_known, type="topNList",n=10)

saveRDS(pred_pop2,"pred_pop2.rds")
saveRDS(pred_pop3,"pred_pop3.rds")
saveRDS(pred_pop5,"pred_pop5.rds")
saveRDS(pred_pop10,"pred_pop10.rds")

# Evaluting the Result of UBCF
error.ubcf2<-calcPredictionAccuracy(pred_ubcf2, ratings_test_unknown,given=0)
error.ubcf3<-calcPredictionAccuracy(pred_ubcf3, ratings_test_unknown,given=0)
error.ubcf5<-calcPredictionAccuracy(pred_ubcf5, ratings_test_unknown,given=0)
error.ubcf10<-calcPredictionAccuracy(pred_ubcf10, ratings_test_unknown,given=0)

# Evaluting the Result of Popular predictor
error.pop2<-calcPredictionAccuracy(pred_pop2, ratings_test_unknown,given=0)
error.pop3<-calcPredictionAccuracy(pred_pop3, ratings_test_unknown,given=0)
error.pop5<-calcPredictionAccuracy(pred_pop5, ratings_test_unknown,given=0)
error.pop10<-calcPredictionAccuracy(pred_pop10, ratings_test_unknown,given=0)
# error.ibcf<-calcPredictionAccuracy(p.ibcf, ratings_test_unknown)

error <- rbind(error.ubcf2,error.ubcf3,error.ubcf5,error.ubcf10,
               error.pop2,error.pop3,error.pop5,error.pop10) %>% cbind("TopN"=c(2,3,5,10,2,3,5,10),"Type"=c("UBCF","UBCF","UBCF","UBCF","POP","POP","POP","POP")) 
```

**Precision-Recall**
Below plot displays the Precesion and Recall of the POPULAR and UBCF Model, We are looking for higher recall with maximum precision, and to our surprise POPULAR (Popular items/Business in our case) MODEL is doing well compare to UBCF. It could be due to the SAMPLE size. 

```{r}
as_data_frame(error) %>% ggplot(aes(recall, precision,color=Type))+
  geom_line() +
  geom_label(aes(label = TopN))  +
  labs(title = "Precision-Recall for Top N", colour = "Model") +
  theme_grey(base_size = 14)+
theme(axis.text.x = element_text(angle = 70, hjust = 1))

```

** ROC Curve TPR vs FPR**
Below Graph is showing the how much is TPR i.e TRUE Positive (Valid Item/business is getting recommended ,which are more likely to be visited by users) vs same amount of FPR i.e not a relevent recommndation . In this case Our focus is to find Max TPR for any given lvel of FPR.

```{r}
as_data_frame(error) %>% ggplot(aes(FPR, TPR,color=Type))+
  geom_line() +
  geom_label(aes(label = TopN))  +
  labs(title = "ROC Curve / TPR vs FPR", colour = "Model") +
  theme_grey(base_size = 14)+
theme(axis.text.x = element_text(angle = 70, hjust = 1))

```

### Predictions For a New User
He we will build the persona of User by visting to a Business (`business_id`)

Function to find top 10 Business for the Given Business. 

```{r include=FALSE}
# Predictions For a New User
REC_TOP_BUSINESS <- function(bus_id = "usAsSV36QmUej8--yvN-dg") {
  

# Generating a prediction with the best performing model. 

# Creating an user visiting to a business :
# # customer_visit <- c("usAsSV36QmUej8--yvN-dg", # Food, Grocery--Phoenix
#                      "PzOqRohWw7F7YEPBz6AubA", # Food, Bagels, Delis, Restaurants--Glendale Az
#                      "JxVGJ9Nly2FFIs_WpJvkug") # Pizza, Restaurants--Scottsdale

customer_visit <- c(bus_id)


# put this order in a format that recommenderlab accept: As shown below
# --5jkZ3-nUPZxUvtcbr8Uw --BlvDO_RG2yElKu9XA1_g -_JBgygYYD_UkuD-GVTp6A .....
#                      0                      0                      0 .....

new_cust_rat_matrx <- df[,c(11,1,13)] %>%  # Select only needed variables
                    # Add a column of 1s an 0s
# Select Business Name
  select(business_id) %>% 
  unique() %>% 
# Add a 'value' column with 1's for Users Visited Business
  mutate(value = as.numeric(business_id %in% customer_visit)) %>% 
# Spread into sparse matrix format
  spread(key = business_id, value = value) %>% 
# Change to a matrix
  as.matrix() %>% 
# Convert to recommenderlab class 'binaryRatingsMatrix'
  as("binaryRatingMatrix")  


# Creating UBCF Model again:
recomm <- Recommender(getData(evalu_a2, 'train'),method  = "POPULAR")
recomm

#I can pass the Recommender and the made-up user's visits  to the predict function to create a top 10 recommendation list for the new customer.
pred <- predict(recomm, 
                newdata = new_cust_rat_matrx, 
                n       = 10)

pred_result <- data.frame(as(pred, 'list') )  %>% rename('business_id'=X1) %>% left_join(df[,c('business_id','categories','city','name')],by='business_id') %>% unique()

pred_result
  
}
```

```{r}
suppressWarnings(REC_TOP_BUSINESS("SvdlC39JGPI_Tj3pS0ruzw"))
```

# Conclusion

**Analysis:**
Given the size of our data, Spark performed the fastest. However, our selected algorithms yielded very similiar results. 

**Limitations:**
The size of our data significantly limited our performance using certain packages in R. Functions in Recommenderlab took ~15 minutes to run in comparision to Sparklyr, which took approximately ~2 minutes. Sparklyr would have been able to handle our full dataset, whereas our personal computers would have lacked the computational memory when solely using Recommenderlab. 

Cold start issue.

Binarymatrix can be best evaluted with the help of Precision and Recall, here Precison stands for predicting Business which is more likely to get the Visits. Precision shows how sensitive models are to False Positives (i.e. recommending a Busniess not very likely to be visited) 

We noted the POPULAR Item method (POPULAR) model gave good result with high recall and same level of Precisions. When compared with all the models for n = 1,2,3,10 for top n items, as number of top n increases we  noted Decrease in the Precison and increase in Recall, since Recall is sesitive to False Negatives (i.e. do not suggest an Business which is highly likely to be visited), so in contrast we aim for higher Recall with maxium Precision.


**Recommendations:**
We would recommend for future attempts performing natural language processing on the review text sentiment and analyzing the term-frequency of our categories to see how these variables could improve our recommendations.

-------

# References

* [**Data Overview**: ](https://www.kaggle.com/c/yelp-recsys-2013/overview) Kagle Yelp Challenge 2013
* [**AR Not supported for binaryRatingMatrix**](https://github.com/mhahsler/recommenderlab/issues/29)
* [**Accuracy Code**](https://github.com/mhahsler/recommenderlab/blob/master/R/calcPredictionAccuracy.R)
* [**Precision vs Recall acceptable Limits**](https://stats.stackexchange.com/questions/323154/precision-vs-recall-acceptable-limits)
* [**Recommenderlab Package Vignette**](https://cran.r-project.org/web/packages/recommenderlab/vignettes/recommenderlab.pdf)