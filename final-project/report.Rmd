---
title: "Final Project - Yelp Recommender System"
author: "Juliann McEachern, Rajwant Mishra,Christina Valore"
date:  " July 16, 2019"
output: 
  html_document:
    theme: paper
    highlight: pygments
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged
    code_folding: hide
---
# Overview 

Our project used data from Kaggle's 2013 Yelp Challenge. This challenge included a subset of Yelp data from the metropolitan area of Phoenix, Arizona. Our data takes into account user reviews, ratings, and check-in data for a wide-range of businesses. 

```{r dependencies,echo=F,comment=F,message=F,warning=F,prompt=F}
## data processing packages
library(dplyr)

##formatting packages
library(knitr); library(kableExtra); library(default)

##visualization packages
library(ggplot2)

##recommender packages
library(recommenderlab); library(Metrics); library(lsa); library(diveRsity)


##preprocessing data
try(setwd("~/GitHub/612-group/final-project"))
suppressWarnings(source("preprocessing.R"))

# global options

## knit sizing
options(max.print="100"); opts_knit$set(width=75) 

## augment chunk output
opts_chunk$set(echo=T,cache=F, tidy=T,comment=F,message=T,warning=T) #change message/warning to F upon completion

## set table style for consistency
default(kable) <- list(format="html")
default(kable_styling)  <- list(bootstrap_options = "hover",full_width=T, font_size=10)
default(scroll_box) <- list(width = "100%")
```


## Data Aquisition & Transformations {.tabset .tabset-fade .tabset-pills}

Data was acquired and transformed in the `preprocessing.R` file located within our repositories final-project folder. Our data source was provided as multiarray Json files, meaning each file is a collection of json data. We used  `stream_in` function, which parses json data line-by-line from the data folder of our repository. The collections included three, large data for Yelp businesses, users, and reviews. 

Once obtained, we prepared our data for our recommender system using the following transformations:

### Business

We choose to limit the scope to our recommender system to only businesses with tags related to food and beverages. There were originally 508 unique category tags listed within our business data. We manually filtered 112 targeted categories to subset our data. 

We applied additional transformation to remove unnessacary data. There were 1,224 business in our data that were permanently closed. These companies accounted for 9.8% of all businesses, which were subsequently removed from our data. There were also 3 businesses in our dataset from outside of AZ that we also removed. 

As a result of our transformations, our recommender data was shortened 4,828 unique businesses. This was further limited to 4,332 after randomly sampling our user-data. The output of which can be previewed below: 

```{r business, echo=F}
#preview business
business %>% head() %>% kable(caption="Preview Business Data") %>% kable_styling()
```

### Review

We subset our review data from the subset of food and beverage businesses. This dropped our review data from 229,907 to 165,823 reviews. We later applied another filter to the data to only use reviews from 10,000 randomly sampled users. This further decreases reviews to 44,494 observations. Our review data can be previewed in two parts below:

```{r review, echo=F}
#preview reviews
review %>% select(-text) %>% head() %>% kable(caption="Preview Review Data (without Review Text)") %>% kable_styling()

#preview review text

review %>% select(text) %>% head(1) %>% kable(caption="Preview of a Singular Review Text") %>% kable_styling()
```

### User 

Last, we applied a similar filter to users to subset our data based on only our selected businesses. This decreased our user data from 43,873 to 35,268 distinct user_id observations. Do to processing constraints in R, we choose to randomly sample 10,000 users from these unique profiles. 

The dataframe preview below shows aggregate user data for all reviews an individual user provided for yelp within our data selection. 

```{r user, echo=F}
#preview users
user %>% head() %>% kable(caption="Preview User Data") %>% kable_styling()
```

## Merge Data 

Next, we created our main dataframe by merging business and reviews on `Business_ID`. This dataframe will serve as the source of data for our recommender algorithms. The user and business unique keys were simplified from characters to numeric user/item identifiers. 

This dataframe will be referenced later on when building our recommender matrices and algorithms. Review details were omitted in the preview for brevity. 

```{r df, echo=F}
#preview main df
df %>% select(-text) %>% head() %>% kable(caption="Preview main dataframe") %>% kable_styling()
```

## Visualize Data

Add data visualizations. 

# Recommender Algorithm

We tested 3 recommender algorithms to see which had the best performance metrics for our recommender system. To test the algorithsm, we first had to create a user-item matrix and then split our data into training and test sets. 

**Matrix Building**

We converted our raw ratings data into a user-item matrix to test and train our subsequent recommender system algorithms. The matrix was saved as a realRatingMatrix for processing purposes later on using the `recommenderlab` package. 

The matrix data can be viewed below. 

```{r matrix-building}
# spread data from long to wide format 
matrix_data <- df %>% select(userID, itemID, stars) %>% spread(itemID, stars)
# set row names to userid
rownames(matrix_data)<-matrix_data$userID 
# remove userid from columns 
matrix_data <-matrix_data %>% select(-userID) 
# convert to matrix
ui_mat <- matrix_data %>% as.matrix()
# store matrix as realRatingMatrix
ui_mat <- as(ui_mat,"realRatingMatrix")
# view matrix data 
matrix_data
```

**Train and Test Splits**

Our data was split into training and tests sets for model evaluation of both two recommender algorithms. We split our data with 10 k-folds using the `recommenderlab` package. 80% of data was retained for training and 20% for testing purposes.

```{r train-test}
# evaluation method with 80% of data for train and 20% for test
set.seed(1000)

evalu <- evaluationScheme(ui_mat, method="split", train=0.8, given=1, goodRating=1, k=10)

# Prep data
train <- getData(evalu, 'train')# Training Dataset 
dev_test <- getData(evalu, 'known') # Test data from evaluationScheme of type KNOWN
test <- getData(evalu, 'unknown') # Unknow datset used for RMSE / model evaluation
```

## Algorithm 1 (Raj)

```{r eval=FALSE, include=FALSE}

evalu_a2 <- evaluationScheme(ui_mat, method="split", train=0.1, given=0, goodRating=1, k=10)
evalu_a2t <- evaluationScheme(ui_mat, method="split", train=0.95, given=0, goodRating=1, k=10)

# Prep data
ratings_train <- getData(evalu_a2, "train")  # Training Dataset 
ratings_test_known <- getData(evalu_a2, "known")  # Test data from evaluationScheme of type KNOWN
ratings_test_unknown <- getData(evalu_a2, "unknown")  # Unknow datset used for RMSE / model


# Prep data
ratings_train_t <- getData(evalu_a2t, "train")  # Training Dataset 
ratings_test_known_t <- getData(evalu_a2t, "known")  # Test data from evaluationScheme of type KNOWN
ratings_test_unknown_t <- getData(evalu_a2t, "unknown")  # Unknow datset used for RMSE / model


# NORMALIZED Train Model
svdn <- Recommender(data = ratings_train, method = "SVD", param = list(k = 10, 
    normalize = "center"))
svdz <- Recommender(data = ratings_train, method = "SVD", param = list(k = 10, 
    normalize = "Z-score"))

## Test/Evaluation Model Predication
svd_predict <- predict(svdn, ratings_test_known_t, type = "ratings")
svd_pred <- calcPredictionAccuracy(svd_predict, ratings_test_unknown_t)
# svdz_predict <- predict(svdz, ratings_test_known, type = "ratings")
# svdz_pred <- calcPredictionAccuracy(svdz_predict, ratings_test_unknown)
# fsvd_predict <- predict(fsvd, ratings_test_known, verbose = F)
# fsvd_predict <- as(fsvd_predict, "realRatingMatrix")
# fsvd_pred = calcPredictionAccuracy(fsvd_predict, ratings_test_unknown)
```

```{r eval=FALSE, include=FALSE}
# I am planning to see how I can use Business's rating and rev_funny,rev_useful,rev_cool
# and see how users are rating against these parameters, I would check cosine similarlty of user 
# rating with these info and recommend some similar  Business to the Users.
head(matrix_data)
head(df)

df_2 <- df[,c(11,1,13)]

# I first convert the ratings into a binary format to keep things simple. ratings of 4 and 5 are 
# mapped to 1, representing likes, and ratings of 3 and below are mapped to -1, representing dislikes.



# as(df_2,"realRatingMatrix") %>%

# Columns are Busienss ID and Row is User_id
binaryratings <- df[,c(11,1,13)] %>%  mutate(Type = case_when(stars == 1 ~ -1,
                                       stars == 2 ~ -1,
                                       stars == 3 ~ -1,
                                       stars == 4 ~ 1,
                                       stars == 5 ~ 1,
                                       TRUE ~ 0)) %>% .[c(1,2,4)] %>%
                                reshape2::dcast(., user_id~business_id, value.var = "Type", na.rm=FALSE)


# search("9Ep4sguv3HH_8lWyzSogjw","-9mj_tVyNkDGF0KWhG19MQ")
# 
# which(binaryratings2 == "9Ep4sguv3HH_8lWyzSogjw", arr.ind = T)
# 
# binaryratings2[91,]

#This basically transforms the data from a long format to a wide format with many NA values, as not every user rated every business, replce NA values with 0.

# for (i in 1:ncol(binaryratings)){
#   binaryratings[which(is.na(binaryratings[,i]) == TRUE),i] <- 0
# }

head(binaryratings)
# binaryratings2 = binaryratings2[,-1] #remove movieIds col. Rows are movieIds, cols are userIds


##_---------------------------------MAIN TASK


evalu_binary <- evaluationScheme(ui_mat, method="split", train=0.3, given=1, goodRating=1, k=10)

# Prep data
train_binary<- getData(evalu_binary, 'train')# Training Dataset 
dev_test_binary <- getData(evalu_binary, 'known') # Test data from evaluationScheme of type KNOWN
test_binary <- getData(evalu_binary, 'unknown') # Unknow datset used for RMSE / model evaluation

#Normalize the data
ratingmat_norm <- Recommender(data = train_binary, method = "SVD", param = list(k = 1, 
    normalize = "center"))


```

Now that we have the user and Business Rating adjsusted where 0 indicates No Feedback, -1 Indicates Negative Feedback and 1 indicates postive feedback.

I decided to use Jaccard Distance to measure the similarity between Busienss profiles, 

```{r eval=FALSE, include=FALSE}
head(binaryratings[,-1])
library(proxy)
 # Sample of Jacard distance on head data.
dist(head(binaryratings[,-1]), method = "Jaccard")
#Calculate Jaccard distance between Buinesses and all Users Reviews

sim_results <- binaryratings[,-1] %>% dist( method = "Jaccard")
sim_results <- as.data.frame(as.matrix(sim_results[1:8552]))
rows <- which(sim_results == min(sim_results))



```


## Algorithm 2 (Christina)

Algo

## Algorithm 3 (Juliann)

Spark requires user/item ids to be numeric. Tidied data accordingly for proccessing. Retained unique keys. May be something to conisder doing in the initial transformations but this will do for now.

```{r}
#install.packages("devtools")
#devtools::install_github("rstudio/sparklyr")
#spark_install(version = "2.3.0")

config <- spark_config()
config$spark.executor.memory <- "8G"
config$spark.executor.cores <- 2
config$spark.executor.instances <- 3
config$spark.dynamicAllocation.enabled <- "false"

sc <- spark_connect(master = "local", config=config, version = "2.1.0")

spark_version(sc)


spark_data <- df %>% select(userID, itemID, stars) 

user_item_ratings <- sdf_copy_to(sc, spark_data, "user_item_ratings", overwrite = TRUE) 

user_item_ratings <- user_item_ratings %>%
  ft_string_indexer(input_col = "userID", output_col = "user_index") %>%
  ft_string_indexer(input_col = "itemID", output_col = "item_index")

partition <- sdf_random_split(user_item_ratings, training = 0.8, testing = 0.2)
sdf_register(partition, c("spark_train", "spark_test"))
  
tidy_train <- tbl(sc, "spark_train") %>%
  select(user_index, item_index, stars)

sparkALS <- ml_als(tidy_train, max_iter = 5, nonnegative = TRUE, 
                   rating_col = "stars", user_col = "user_index", item_col = "item_index")

spark_test <- tbl(sc, "spark_test")

predictions <- ml_predict(sparkALS, spark_test)
prediction <- collect(predictions)
head(prediction)
```

Train Test Splits 

```{r, eval=FALSE, include=FALSE}
user_item_ratings %>% 
  
  

item <- ft_string_indexer(sc,input_col="ProductID",output_col="product_index")
user <- ft_string_indexer(sc,input_col="UserID",output_col="user_index")

# Import data to Spark cluster


src_tbls(sc)

is_ml_estimator(sprk_tbl$item)
```

```{r, eval=FALSE, include=FALSE}
model <- ml_als(sprk_df, rating_col = "rating", user_col =  "user", item_col = "item")
```





```{r, eval=FALSE, include=FALSE}
# Partition train/test data sets 
partitions <- sdf_random_split(sprk_df, training = 0.8, test = 0.2, seed = 1000)

# Set train/test data
train <- partitions$training
test <- partitions$test

# View data split size for validation (delete later)
count(train)
count(test)

#fit <- ml_als(train, rating_col= "rating", user_col = "user", item_col= "item")
```

Modeling section. 



## Analysis

Compare algorithms performance. Select most effective to build recommender system.

# Recommender System

Test system

# Conclusion

Final conlusion. Explain limitations of system. Make recommendations for future improvements.  

# References
* [**Data Overview**: ](https://www.kaggle.com/c/yelp-recsys-2013/overview) Kagle Yelp Challenge 2013

