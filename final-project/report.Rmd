---
title: "Final Project - Yelp Recommender System"
author: "Juliann McEachern, Rajwant Mishra,Christina Valore"
date:  " July 16, 2019"
output: 
  html_document:
    theme: paper
    highlight: pygments
    toc: true
    toc_float: true
    toc_depth: 2
    df_print: paged
    code_folding: hide
---
# Overview 

Our project used data from Kaggle's 2013 Yelp Challenge. This challenge included a subset of Yelp data from the metropolitan area of Phoenix, Arizona. Our data takes into account user reviews, ratings, and check-in data for a wide-range of businesses. 

```{r dependencies,echo=F,comment=F,message=F,warning=F,prompt=F}
## data processing packages
library(dplyr);library(tibble);library(forcats)

##formatting packages
library(knitr); library(kableExtra); library(default)

##visualization packages
library(ggplot2)

##recommender packages
library(recommenderlab); library(Metrics); library(lsa); library(diveRsity);library(purrr)

## Sparklyr 
#install.packages("devtools")
#devtools::install_github("rstudio/sparklyr")
#spark_install(version = "2.4.3")
library(sparklyr)

##preprocessing data
try(setwd("~/612-group/final-project"))
suppressWarnings(source("preprocessing.R"))

# global options

## knit sizing
options(max.print="100"); opts_knit$set(width=75) 

## augment chunk output
opts_chunk$set(echo=T,cache=F, tidy=T,comment=F,message=T,warning=T) #change message/warning to F upon completion

## set table style for consistency
default(kable) <- list(format="html")
default(kable_styling)  <- list(bootstrap_options = "hover",full_width=T, font_size=10)
default(scroll_box) <- list(width = "100%")
```


## Data Aquisition & Transformations {.tabset .tabset-fade .tabset-pills}

Data was acquired and transformed in the `preprocessing.R` file located within our repositories final-project folder. Our data source was provided as multiarray Json files, meaning each file is a collection of json data. We used  `stream_in` function, which parses json data line-by-line from the data folder of our repository. The collections included three, large data for Yelp businesses, users, and reviews. 

Once obtained, we prepared our data for our recommender system using the following transformations:

### Business

We choose to limit the scope to our recommender system to only businesses with tags related to food and beverages. There were originally 508 unique category tags listed within our business data. We manually filtered 112 targeted categories to subset our data. 

We applied additional transformation to remove unnessacary data. There were 1,224 business in our data that were permanently closed. These companies accounted for 9.8% of all businesses, which were subsequently removed from our data. There were also 3 businesses in our dataset from outside of AZ that we also removed. 

As a result of our transformations, our recommender data was shortened 4,828 unique businesses. This was further limited to 4,332 after randomly sampling our user-data. The output of which can be previewed below: 

```{r business, echo=F}
#preview business
business %>% head() %>% kable(caption="Preview Business Data") %>% kable_styling()
```

### Review

We subset our review data from the subset of food and beverage businesses. This dropped our review data from 229,907 to 165,823 reviews. We later applied another filter to the data to only use reviews from 10,000 randomly sampled users. This further decreases reviews to 44,494 observations. Our review data can be previewed in two parts below:

```{r review, echo=F}
#preview reviews
review %>% select() %>% head(1) %>% kable(caption="Preview Review Data (without Review Text)") %>% kable_styling()
```

### User 

Last, we applied a similar filter to users to subset our data based on only our selected businesses. This decreased our user data from 43,873 to 35,268 distinct user_id observations. Do to processing constraints in R, we choose to randomly sample 10,000 users from these unique profiles. 

The dataframe preview below shows aggregate user data for all reviews an individual user provided for yelp within our data selection. 

```{r user, echo=F}
#preview users
user %>% head() %>% kable(caption="Preview User Data") %>% kable_styling()
```

## Merge Data 

Next, we created our main dataframe by merging business and reviews on `Business_ID`. This dataframe will serve as the source of data for our recommender algorithms. The user and business unique keys were simplified from characters to numeric user/item identifiers. 

This dataframe will be referenced later on when building our recommender matrices and algorithms. Review details were omitted in the preview for brevity. 

```{r df, echo=F}
df %>% select(-text) %>% head() %>% kable(caption="Preview main dataframe") %>% kable_styling()
```

## Visualize Data

Add data visualizations. 

# Recommender Algorithm

We tested recommender algorithms using `recommenderlab` and `sparklyr` to see which performed the best on our recommender system data. To test the algorithsm, we first had to create a user-item matrix and then split our data into training and test sets. 

**Matrix Building**

We converted our raw ratings data into a user-item matrix to test and train our subsequent recommender system algorithms. The matrix was saved as a realRatingMatrix for processing purposes later on using the `recommenderlab` package. 

The matrix data can be viewed below. 

```{r matrix-building}
# spread data from long to wide format 
matrix_data <- df %>% select(userID, itemID, stars) %>% spread(itemID, stars)
# set row names to userid
rownames(matrix_data)<-matrix_data$userID 
# remove userid from columns 
matrix_data <-matrix_data %>% select(-userID) 
# convert to matrix
ui_mat <- matrix_data %>% as.matrix()
# store matrix as realRatingMatrix
ui_mat <- as(ui_mat,"realRatingMatrix")
# view matrix data 
matrix_data
```

**Train and Test Splits**

Our data was split into training and tests sets for model evaluation of both two recommender algorithms. We split our data with 10 k-folds using the `recommenderlab` package. 80% of data was retained for training and 20% for testing purposes.

```{r train-test}
# evaluation method with 80% of data for train and 20% for test
set.seed(1000)

evalu <- evaluationScheme(ui_mat, method="split", train=0.8, given=1, goodRating=1, k=10)

# Prep data
train <- getData(evalu, 'train')# Training Dataset 
dev_test <- getData(evalu, 'known') # Test data from evaluationScheme of type KNOWN
test <- getData(evalu, 'unknown') # Unknow datset used for RMSE / model evaluation
```


## RecommenderLab {.tabset .tabset-fade .tabset-pills}

### User-based CF

```{r}
UB <- Recommender(getData(evalu, "train"), "UBCF", 
      param=list(normalize = "Z-score",method="Cosine"))

p <- predict(UB, getData(evalu, "known"), type="ratings")

p@data@x[p@data@x[] < 1] <- 1
p@data@x[p@data@x[] > 5] <- 5

calcPredictionAccuracy(p, getData(evalu, "unknown"))
```

### Item-based CF

```{r}
IB <- Recommender(getData(evalu, "train"), "IBCF", 
      param=list(normalize = "Z-score",method="Cosine"))

p1 <- predict(IB, getData(evalu, "known"), type="ratings")

p1@data@x[p1@data@x[] < 1] <- 1
p1@data@x[p1@data@x[] > 5] <- 5

calcPredictionAccuracy(p1, getData(evalu, "unknown"))
```

## Sparklyr {.tabset .tabset-fade .tabset-pills}

Due to the size of our data, we choose to use Spark in R to avoid input/output (I/O) bottleneck issues and maximize the performance speed of our recommender algorithm calculations.

### Connecting

We initiated a local connection with Spark (V2.4.3). Our yelp data was inputted into a spark table and split for training and testing purposes. 

```{r initiate-spark}
# configure spark connection
config <- spark_config()
config$spark.executor.memory <- "8G"
config$spark.executor.cores <- 2
config$spark.executor.instances <- 3
config$spark.dynamicAllocation.enabled <- "false"

# initiate connection
sc <- spark_connect(master = "local", config=config, version = "2.4.3")

# unhash to verify version: 
# spark_version(sc)

# select data for spark and create spark table 
spark_data <- df %>% select(stars, user_id, business_id, name, city, categories)

yelp <- sdf_copy_to(sc, spark_data, "yelp", overwrite = TRUE) 

# Transform features
yelp <- yelp %>%
  ft_string_indexer(input_col = "user_id", output_col = "user_index") %>%
  ft_string_indexer(input_col = "business_id", output_col = "item_index") %>%
  select(-user_id, -business_id)%>%
  sdf_register("yelp")

# randomly split / train test data
split <- sdf_random_split(yelp, training = 0.8, testing = 0.2, seed=1)

# store training / test sets
train <- sdf_register(split$training, "train")
test <- sdf_register(split$testing, "test")

# tidy train for algoritms that require only user/item inputs
ui_train <- tbl(sc, "train") %>% select(user_index, item_index, stars)
ui_test <- tbl(sc, "test") %>% select(user_index, item_index, stars)
```


### ALS 

Once connected, we applied the alternating least squares (ALS) for our recommender predictions.

```{r spark-als}
# build model using user/business/ratings
als_fit <- ml_als(ui_train, max_iter = 5, nonnegative = TRUE, 
                   rating_col = "stars", 
                   user_col = "user_index", 
                   item_col = "item_index")

# predict from the model for the training data
als_predict_train <- ml_predict(als_fit, ui_train) %>% collect()
als_predict_test <- ml_predict(als_fit, ui_test) %>% collect()

# Remove NaN (result of test/train splits - not data)
als_predict_train <- als_predict_train[!is.na(als_predict_train$prediction), ] 
als_predict_test <- als_predict_test[!is.na(als_predict_test$prediction), ] 

# View results
als_predict_test %>% head %>% kable() %>% kable_styling()
```

### Metrics 

Our ALS calculations for RMSE, MSE, and MAE can be viewed below: 

```{r}
# Calculate RMSE/MSE/MAE 
als_mse_train <- mean((als_predict_train$stars - als_predict_train$prediction)^2)
als_rmse_train <- sqrt(als_mse_train)
als_mae_train <- mean(abs(als_predict_train$stars - als_predict_train$prediction))

als_mse_test <- mean((als_predict_test$stars - als_predict_test$prediction)^2)
als_rmse_test <- sqrt(als_mse_test)
als_mae_test <- mean(abs(als_predict_test$stars - als_predict_test$prediction))

# View metrics 
cbind(als_rmse_train,als_mse_train, als_mae_train) 
cbind(als_rmse_test, als_mse_test, als_mae_test) 
```


## Random Forest

We wanted to test out other machine learning options in Spark. We tried using random forest decision trees, however this method yielded very low accuracy on our training data. This method would not produce a good recommender system for businesses or users in our dataset.  

```{r rf}
# build model using user/business index, category, and city
rf_fit <- ml_random_forest(
  train, # the training partion
  response = "stars",
  features = colnames(train)[2:4], 
  impurity = "entropy",
  type = "classification",
  seed = 1)

# identify important features in model
rf_importance1 <- ml_tree_feature_importance(sc = sc, model = rf_fit) 
rf_importance2<- rf_importance1 %>% mutate(importance = round(importance,2)) %>% filter(importance>0) 

# percent of terms found important

nrow(rf_importance2)/nrow(rf_importance1) 

# view importance
rf_importance2 %>% kable() %>% kable_styling()
```

Accuracy metrics for Random Forest: 

```{r rf-metrics}
# make predictions 
rf_predict_train <- ml_predict(rf_fit, train)  
rf_predict_test <- ml_predict(rf_fit, test)


# calculate accuracy 
rf_eval_train <- rf_predict_train %>% 
  ml_multiclass_classification_evaluator(label = "stars", metric = "accuracy") 

##UNABLE TO GET TEST ACCURACY
# rf_eval_test <- rf_predict_test %>% 
# ml_multiclass_classification_evaluator(label = "stars", metric = "accuracy") 

rf_eval_train
```

```{r rf-rmse, include=F, eval=F}
## NOT INCLUDED: UNABLE TO COLLECT PREDICTIONS FOR RMSE/ETC COMPARISION
# predict from the model for the training data
rf_predict_train <- ml_predict(rf_fit, train)  %>% collect()
rf_predict_test <- ml_predict(rf_fit, test) %>% collect()

# Remove NaN (result of test/train splits - not data)
rf_predict_train <- rf_predict_train[!is.na(rf_predict_train$prediction), ] 
rf_predict_test <- rf_predict_test[!is.na(rf_predict_test$prediction), ] 

# Calculate MSE/RMSE/MAE 
rf_mse_train <- mean((rf_predict_train$stars - rf_predict_train$prediction)^2)
rf_rmse_train <- sqrt(rf_mse_train)
rf_mae_train <- mean(abs(rf_predict_train$stars - rf_predict_train$prediction))

rf_mse_test <- mean((rf_predict_test$stars - rf_predict_test$prediction)^2)
rf_rmse_test <- sqrt(rf_mse_test)
rf_mae_test <- mean(abs(rf_predict_test$stars - rf_predict_test$prediction))

#rbind(rf_mse_train, rf_rmse_train, rf_mae_train) 
#rbind(rf_mse_test, rf_rmse_test, rf_mae_test) 
```

```{r disconnect, echo=F}
# disconnect
spark_disconnect(sc)
```

## Algorithm with Binary Rating  {.tabset .tabset-fade .tabset-pills}

+ In the approach we will work with Bianry dataset and see how Business can be Recommended for the Given Business.
+ We wil Group our Rating in two categroy , 1-3 will be set as `0` and rest is `1`


### Binary Rating 
```{r}

ratings_matrix  = df[,c(11,1,13)] %>%  # Select only needed variables
                    # Add a column of 1s an 0s
                    mutate(Type = case_when(stars == 1 ~ 0,
                                       stars == 2 ~ 0,
                                       stars == 3 ~ 0,
                                       stars == 4 ~ 1,
                                       stars == 5 ~ 1,
                                       TRUE ~ 0)) %>% .[c(1,2,4)] %>% 
                    # Convert to recommenderlab class 'binaryRatingsMatrix'
                    as("binaryRatingMatrix")

```


We have splited the data into a train and a test set by selecting train = 0.8 for a 80/20 train/test split. I have also set method = “cross” and k = 5 for a 5-fold cross validation. This means that the data is divided into k subsets of equal size, with 80% of the data used for training and the remaining 20% used for evaluation. The models are recursively estimated 5 times, each time using a different train/test split, which ensures that all users and items are considered for both training and testing. The results can then be averaged to produce a single evaluation set.

Note: Selecting given = -1 means that for the test users ‘all but 1’ randomly selected item is withheld for evaluation.

```{r}
scheme <- ratings_matrix %>% 
  evaluationScheme(method = "cross",
                   k      = 5, 
                   train  = 0.8,  
                   given  = -1)

scheme 
```

### Set up List of Algorithms
Recommenderlab  gives the ability to estimate multiple algorithms in one go. First, I created a list with the algorithms I want to estimate, specifying all the models parameters. Here, I consider schemes which evaluate on a binary rating matrix. 

```{r eval=FALSE, include=FALSE}
algorithms <- list(
  # "association rules" = list(name  = "AR", 
  #                       param = list(supp = 0.01, conf = 0.01)), # Binary Matrix is not supported
  "random items"      = list(name  = "RANDOM",  param = NULL),
  "popular items"     = list(name  = "POPULAR", param = NULL),
  "item-based CF"     = list(name  = "IBCF", param = list(k = 5)),
  "user-based CF"     = list(name  = "UBCF", 
                        param = list(method = "Cosine", nn = 500))
                   )
```

### Estimate the Models
All I have to do now is to pass scheme and algoritms to the evaluate() function, select type = topNList to evaluate a Top N List of product recommendations and specify how many recommendations to calculate with the parameter n = c(1, 3, 5, 10, 15, 20).

Note: Model was saved to file system after running it once to save time in next run. 
```{r eval=FALSE, include=FALSE}
results <- recommenderlab::evaluate(scheme, 
                                    algorithms, 
                                    type  = "topNList", 
                                    n     = c(1, 3, 5, 10, 15, 20)
                                    )

saveRDS(result,"resulta2.rds")
```

### Visualising The Model
```{r}
# Pull into a list all confusion matrix information for one model 
result = readRDS("resulta2.rds")
tmp <- results$`user-based CF` %>%
  getConfusionMatrix()  %>%  
  as.list() 

# Calculate average value of 5 cross-validation rounds 
  as.data.frame( Reduce("+",tmp) / length(tmp)) %>% 
# Add a column to mark the number of recommendations calculated
  mutate(n = c(1, 3, 5, 10, 15, 20)) %>%
# Select only columns needed and sorting out order 
  select('n', 'precision', 'recall', 'TPR', 'FPR')
  
  
avg_conf_matr <- function(results) {
  tmp <- results %>%
    getConfusionMatrix()  %>%  
    as.list() 
    as.data.frame(Reduce("+",tmp) / length(tmp)) %>% 
    mutate(n = c(1, 3, 5, 10, 15, 20)) %>%
    select('n', 'precision', 'recall', 'TPR', 'FPR') 
}
library(purrr)
library(tibble)
library(forcats)
# Using map() to iterate function across all models
results_tbl <- results %>%
  map(avg_conf_matr) %>% 
# Turning into an unnested tibble
  enframe() %>%
# Unnesting to have all variables on same level
  unnest()
results_tbl

```

**ROC curve**
---
 Classification models performance can be compared using the ROC curve, which plots the true positive rate (TPR) against the false positive rate (FPR).
 The User-based collaborative filtering model is the clear winner as it achieves the highest TPR for any given level of FPR. This means that the model is producing the highest number of relevant recommendations (true positives) for the same level of non-relevant recommendations (false positives).
 In theory, if TPR ~1 is Good means close to 100% Accuracy  and if FPR ~0 , Mean number of False Positive business is close to zero. 
For Ref:
  TPR <- recall
  FPR <- FP / (FP + TN)
---

```{r echo=FALSE}

results_tbl %>%
  ggplot(aes(FPR, TPR, 
             colour = fct_reorder2(as.factor(name), 
                      FPR, TPR))) +
  geom_line() +
  geom_label(aes(label = n))  +
  labs(title = "ROC curves", colour = "Model") +
  theme_grey(base_size = 14)
```

**Precision-Recall curve**

Other way to compare classification models performance is with Precision Vs Recall curves. Precision shows how sensitive models are to False Positives (i.e. recommending an Business  not very likely to be visited) whereas Recall (which is just another name for the TPR) looks at how sensitive models are to False Negatives (i.e. do not suggest an Business which is More likely to be visited).
 So we want to maximise Recall (or minimise FNs) for the same level of Precision.
For Ref:
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)

The plot confirms that User-based Collaborative Filter (UBCF) is the best model because it has higher Recall for any given level of Precision. This means that UBCF minimises FNs (i.e. not suggesting an item highly likely to be purchased) for all level of FPs.

```{r echo=FALSE}
results_tbl %>%
  ggplot(aes(recall, precision, 
             colour = fct_reorder2(as.factor(name),  
                      precision, recall))) +
  geom_line() +
  geom_label(aes(label = n))  +
  labs(title = "Precision-Recall curves", colour = "Model") +
  theme_grey(base_size = 14)
```

### Predictions For a New User

He we will build the persona of User by visting to a Business (`business_id`)

Function to fidn top 10 Business for the Given Business. 
```{r echo=TRUE}

REC_TOP_BUSINESS <- function(bus_id = "usAsSV36QmUej8--yvN-dg") {
  

# Generating a prediction with the best performing model. 

# Creating an user visiting to a business :
# # customer_visit <- c("usAsSV36QmUej8--yvN-dg", # Food, Grocery--Phoenix
#                      "PzOqRohWw7F7YEPBz6AubA", # Food, Bagels, Delis, Restaurants--Glendale Az
#                      "JxVGJ9Nly2FFIs_WpJvkug") # Pizza, Restaurants--Scottsdale

customer_visit <- c(bus_id)


# put this order in a format that recommenderlab accept: As shown below
# --5jkZ3-nUPZxUvtcbr8Uw --BlvDO_RG2yElKu9XA1_g -_JBgygYYD_UkuD-GVTp6A .....
#                      0                      0                      0 .....

new_cust_rat_matrx <- df[,c(11,1,13)] %>%  # Select only needed variables
                    # Add a column of 1s an 0s
# Select Business Name
  select(business_id) %>% 
  unique() %>% 
# Add a 'value' column with 1's for Users Visited Business
  mutate(value = as.numeric(business_id %in% customer_visit)) %>% 
# Spread into sparse matrix format
  spread(key = business_id, value = value) %>% 
# Change to a matrix
  as.matrix() %>% 
# Convert to recommenderlab class 'binaryRatingsMatrix'
  as("binaryRatingMatrix")  


# Creating UBCF Model again:
recomm <- Recommender(getData(scheme, 'train'),method  = "UBCF", 
                        param = list(method = "Cosine", nn = 500))
recomm

#I can pass the Recommender and the made-up user's visits  to the predict function to create a top 10 recommendation list for the new customer.
pred <- predict(recomm, 
                newdata = new_cust_rat_matrx, 
                n       = 10)

pred_result <- data.frame(as(pred, 'list') )  %>% rename('business_id'=X1) %>% left_join(df[,c('business_id','categories','city','name')],by='business_id') %>% unique()

pred_result
  
}

```

Finding top 10 for Business simialr to "SvdlC39JGPI_Tj3pS0ruzw"
```{r}

suppressWarnings(REC_TOP_BUSINESS("SvdlC39JGPI_Tj3pS0ruzw"))
```



# Conclusion

Given the size of our data, Spark performed the fastest. However, the results for our three algorithms yielded very similiar results. 


Add more to final conlusion. Explain limitations of system. Make recommendations for future improvements.  

-------

# References

* [**Data Overview**: ](https://www.kaggle.com/c/yelp-recsys-2013/overview) Kagle Yelp Challenge 2013